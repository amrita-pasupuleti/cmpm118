
Epoch: 1
[epoch:1, iter:50] Loss: 2.277 | Acc: 15.109% 
[epoch:1, iter:100] Loss: 2.093 | Acc: 20.727% 
[epoch:1, iter:150] Loss: 2.001 | Acc: 23.984% 
[epoch:1, iter:200] Loss: 1.928 | Acc: 26.867% 
[epoch:1, iter:250] Loss: 1.868 | Acc: 29.172% 
[epoch:1, iter:300] Loss: 1.820 | Acc: 31.128% 
[epoch:1, iter:350] Loss: 1.777 | Acc: 32.877% 
Testing...
Test Accuracy: 40.180%

Epoch: 2
[epoch:2, iter:50] Loss: 1.433 | Acc: 46.922% 
[epoch:2, iter:100] Loss: 1.403 | Acc: 48.484% 
[epoch:2, iter:150] Loss: 1.385 | Acc: 49.026% 
[epoch:2, iter:200] Loss: 1.361 | Acc: 49.801% 
[epoch:2, iter:250] Loss: 1.339 | Acc: 50.638% 
[epoch:2, iter:300] Loss: 1.325 | Acc: 51.151% 
[epoch:2, iter:350] Loss: 1.304 | Acc: 51.913% 
Testing...
Test Accuracy: 46.760%

Epoch: 3
[epoch:3, iter:50] Loss: 1.118 | Acc: 59.875% 
[epoch:3, iter:100] Loss: 1.095 | Acc: 60.430% 
[epoch:3, iter:150] Loss: 1.081 | Acc: 60.969% 
[epoch:3, iter:200] Loss: 1.072 | Acc: 61.289% 
[epoch:3, iter:250] Loss: 1.059 | Acc: 61.788% 
[epoch:3, iter:300] Loss: 1.050 | Acc: 62.122% 
[epoch:3, iter:350] Loss: 1.042 | Acc: 62.406% 
Testing...
Test Accuracy: 59.210%

Epoch: 4
[epoch:4, iter:50] Loss: 0.947 | Acc: 65.594% 
[epoch:4, iter:100] Loss: 0.925 | Acc: 66.656% 
[epoch:4, iter:150] Loss: 0.925 | Acc: 66.943% 
[epoch:4, iter:200] Loss: 0.921 | Acc: 67.062% 
[epoch:4, iter:250] Loss: 0.922 | Acc: 67.037% 
[epoch:4, iter:300] Loss: 0.912 | Acc: 67.346% 
[epoch:4, iter:350] Loss: 0.907 | Acc: 67.625% 
Testing...
Test Accuracy: 65.630%

Epoch: 5
[epoch:5, iter:50] Loss: 0.817 | Acc: 71.328% 
[epoch:5, iter:100] Loss: 0.823 | Acc: 71.133% 
[epoch:5, iter:150] Loss: 0.819 | Acc: 71.010% 
[epoch:5, iter:200] Loss: 0.811 | Acc: 71.426% 
[epoch:5, iter:250] Loss: 0.808 | Acc: 71.566% 
[epoch:5, iter:300] Loss: 0.801 | Acc: 71.737% 
[epoch:5, iter:350] Loss: 0.799 | Acc: 71.768% 
Testing...
Test Accuracy: 67.180%

Epoch: 6
[epoch:6, iter:50] Loss: 0.755 | Acc: 73.625% 
[epoch:6, iter:100] Loss: 0.740 | Acc: 73.938% 
[epoch:6, iter:150] Loss: 0.734 | Acc: 73.906% 
[epoch:6, iter:200] Loss: 0.727 | Acc: 74.141% 
[epoch:6, iter:250] Loss: 0.718 | Acc: 74.459% 
[epoch:6, iter:300] Loss: 0.717 | Acc: 74.565% 
[epoch:6, iter:350] Loss: 0.718 | Acc: 74.612% 
Testing...
Test Accuracy: 74.510%

Epoch: 7
[epoch:7, iter:50] Loss: 0.674 | Acc: 76.156% 
[epoch:7, iter:100] Loss: 0.677 | Acc: 75.953% 
[epoch:7, iter:150] Loss: 0.672 | Acc: 76.161% 
[epoch:7, iter:200] Loss: 0.663 | Acc: 76.477% 
[epoch:7, iter:250] Loss: 0.660 | Acc: 76.703% 
[epoch:7, iter:300] Loss: 0.656 | Acc: 76.932% 
[epoch:7, iter:350] Loss: 0.650 | Acc: 77.069% 
Testing...
Test Accuracy: 77.730%

Epoch: 8
[epoch:8, iter:50] Loss: 0.598 | Acc: 78.984% 
[epoch:8, iter:100] Loss: 0.591 | Acc: 79.148% 
[epoch:8, iter:150] Loss: 0.594 | Acc: 79.083% 
[epoch:8, iter:200] Loss: 0.593 | Acc: 79.207% 
[epoch:8, iter:250] Loss: 0.594 | Acc: 79.200% 
[epoch:8, iter:300] Loss: 0.595 | Acc: 79.151% 
[epoch:8, iter:350] Loss: 0.591 | Acc: 79.317% 
Testing...
Test Accuracy: 78.960%

Epoch: 9
[epoch:9, iter:50] Loss: 0.558 | Acc: 80.828% 
[epoch:9, iter:100] Loss: 0.562 | Acc: 80.797% 
[epoch:9, iter:150] Loss: 0.558 | Acc: 80.698% 
[epoch:9, iter:200] Loss: 0.555 | Acc: 80.844% 
[epoch:9, iter:250] Loss: 0.549 | Acc: 81.041% 
[epoch:9, iter:300] Loss: 0.547 | Acc: 81.068% 
[epoch:9, iter:350] Loss: 0.543 | Acc: 81.156% 
Testing...
Test Accuracy: 78.400%

Epoch: 10
[epoch:10, iter:50] Loss: 0.509 | Acc: 82.156% 
[epoch:10, iter:100] Loss: 0.516 | Acc: 81.719% 
[epoch:10, iter:150] Loss: 0.515 | Acc: 81.844% 
[epoch:10, iter:200] Loss: 0.514 | Acc: 82.004% 
[epoch:10, iter:250] Loss: 0.514 | Acc: 82.047% 
[epoch:10, iter:300] Loss: 0.515 | Acc: 82.029% 
[epoch:10, iter:350] Loss: 0.514 | Acc: 82.105% 
Testing...
Test Accuracy: 79.810%
Training complete.




Network Class:
class ResidualBlock(nn.Module):
    def __init__(self, inchannel, outchannel, stride=1):
        super(ResidualBlock, self).__init__()
        self.left = nn.Sequential(
            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(outchannel),
            nn.ReLU(inplace=True),
            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(outchannel)
        )
        self.shortcut = nn.Sequential()
        if stride != 1 or inchannel != outchannel:
            self.shortcut = nn.Sequential(
                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(outchannel)
            )

    def forward(self, x):
        out = self.left(x)
        out = out + self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet(nn.Module):
    def __init__(self, ResidualBlock, num_classes=10):
        super(ResNet, self).__init__()
        self.inchannel = 64
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),  # Adjusted for RGB images
            nn.BatchNorm2d(64),
            nn.ReLU()
        )
        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)
        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)
        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)
        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)
        
        # Dropout before fully connected layer
        self.dropout = nn.Dropout(p=0.5)  # Set dropout probability (e.g., 0.5)
        self.fc = nn.Linear(512, num_classes)

    def make_layer(self, block, channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.inchannel, channels, stride))
            self.inchannel = channels
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = F.dropout(out, p=0.3, training=self.training)  # Dropout after layer1
        out = self.layer2(out)
        out = F.dropout(out, p=0.3, training=self.training)  # Dropout after layer2
        out = self.layer3(out)
        out = F.dropout(out, p=0.3, training=self.training)  # Dropout after layer3
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        
        # Apply dropout before fully connected layer
        out = self.dropout(out)
        
        out = self.fc(out)
        return out

def ResNet18():
    return ResNet(ResidualBlock)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.95, weight_decay=5e-4)