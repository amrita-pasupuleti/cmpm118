{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.01\n",
    "\n",
    "# Prepare dataset and preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(32),                  # Resize for ResNet\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))    # Normalization for grayscale\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),                  # Resize for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))    # Normalization for grayscale\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "# Labels in Fashion MNIST\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BitLinear layer\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BitLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Initialize weights and biases close to zero\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)\n",
    "        self.bias = nn.Parameter(torch.randn(out_features) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make weights binary\n",
    "        binarized_weight = torch.sign(torch.tanh(self.weight))\n",
    "        return F.linear(x, binarized_weight, self.bias)\n",
    "\n",
    "# Define the Residual Block and ResNet\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),  # Adjusted for grayscale images\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = BitLinear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:50] Loss: 11.153 | Acc: 51.844% \n",
      "[epoch:1, iter:100] Loss: 6.063 | Acc: 61.305% \n",
      "[epoch:1, iter:150] Loss: 4.320 | Acc: 65.005% \n",
      "[epoch:1, iter:200] Loss: 3.438 | Acc: 67.172% \n",
      "[epoch:1, iter:250] Loss: 2.889 | Acc: 69.088% \n",
      "[epoch:1, iter:300] Loss: 2.516 | Acc: 70.396% \n",
      "[epoch:1, iter:350] Loss: 2.240 | Acc: 71.504% \n",
      "[epoch:1, iter:400] Loss: 2.034 | Acc: 72.398% \n",
      "[epoch:1, iter:450] Loss: 1.878 | Acc: 72.946% \n",
      "Testing...\n",
      "Test Accuracy: 78.660%\n",
      "\n",
      "Epoch: 2\n",
      "[epoch:2, iter:50] Loss: 0.606 | Acc: 78.328% \n",
      "[epoch:2, iter:100] Loss: 0.569 | Acc: 79.398% \n",
      "[epoch:2, iter:150] Loss: 0.555 | Acc: 80.000% \n",
      "[epoch:2, iter:200] Loss: 0.541 | Acc: 80.449% \n",
      "[epoch:2, iter:250] Loss: 0.531 | Acc: 80.747% \n",
      "[epoch:2, iter:300] Loss: 0.525 | Acc: 80.984% \n",
      "[epoch:2, iter:350] Loss: 0.521 | Acc: 81.092% \n",
      "[epoch:2, iter:400] Loss: 0.517 | Acc: 81.289% \n",
      "[epoch:2, iter:450] Loss: 0.516 | Acc: 81.264% \n",
      "Testing...\n",
      "Test Accuracy: 82.730%\n",
      "\n",
      "Epoch: 3\n",
      "[epoch:3, iter:50] Loss: 0.466 | Acc: 83.266% \n",
      "[epoch:3, iter:100] Loss: 0.463 | Acc: 82.992% \n",
      "[epoch:3, iter:150] Loss: 0.470 | Acc: 82.854% \n",
      "[epoch:3, iter:200] Loss: 0.472 | Acc: 82.746% \n",
      "[epoch:3, iter:250] Loss: 0.465 | Acc: 82.938% \n",
      "[epoch:3, iter:300] Loss: 0.462 | Acc: 83.141% \n",
      "[epoch:3, iter:350] Loss: 0.461 | Acc: 83.266% \n",
      "[epoch:3, iter:400] Loss: 0.460 | Acc: 83.355% \n",
      "[epoch:3, iter:450] Loss: 0.457 | Acc: 83.444% \n",
      "Testing...\n",
      "Test Accuracy: 80.260%\n",
      "\n",
      "Epoch: 4\n",
      "[epoch:4, iter:50] Loss: 0.445 | Acc: 83.828% \n",
      "[epoch:4, iter:100] Loss: 0.448 | Acc: 83.695% \n",
      "[epoch:4, iter:150] Loss: 0.447 | Acc: 83.917% \n",
      "[epoch:4, iter:200] Loss: 0.438 | Acc: 84.113% \n",
      "[epoch:4, iter:250] Loss: 0.443 | Acc: 83.909% \n",
      "[epoch:4, iter:300] Loss: 0.434 | Acc: 84.089% \n",
      "[epoch:4, iter:350] Loss: 0.432 | Acc: 84.098% \n",
      "[epoch:4, iter:400] Loss: 0.428 | Acc: 84.297% \n",
      "[epoch:4, iter:450] Loss: 0.430 | Acc: 84.314% \n",
      "Testing...\n",
      "Test Accuracy: 83.780%\n",
      "\n",
      "Epoch: 5\n",
      "[epoch:5, iter:50] Loss: 0.386 | Acc: 86.344% \n",
      "[epoch:5, iter:100] Loss: 0.397 | Acc: 85.852% \n",
      "[epoch:5, iter:150] Loss: 0.402 | Acc: 85.656% \n",
      "[epoch:5, iter:200] Loss: 0.398 | Acc: 85.715% \n",
      "[epoch:5, iter:250] Loss: 0.391 | Acc: 85.872% \n",
      "[epoch:5, iter:300] Loss: 0.391 | Acc: 85.911% \n",
      "[epoch:5, iter:350] Loss: 0.391 | Acc: 85.897% \n",
      "[epoch:5, iter:400] Loss: 0.392 | Acc: 85.838% \n",
      "[epoch:5, iter:450] Loss: 0.391 | Acc: 85.872% \n",
      "Testing...\n",
      "Test Accuracy: 84.450%\n",
      "\n",
      "Epoch: 6\n",
      "[epoch:6, iter:50] Loss: 0.377 | Acc: 85.750% \n",
      "[epoch:6, iter:100] Loss: 0.386 | Acc: 85.773% \n",
      "[epoch:6, iter:150] Loss: 0.390 | Acc: 85.688% \n",
      "[epoch:6, iter:200] Loss: 0.386 | Acc: 85.754% \n",
      "[epoch:6, iter:250] Loss: 0.378 | Acc: 86.169% \n",
      "[epoch:6, iter:300] Loss: 0.376 | Acc: 86.260% \n",
      "[epoch:6, iter:350] Loss: 0.373 | Acc: 86.368% \n",
      "[epoch:6, iter:400] Loss: 0.371 | Acc: 86.410% \n",
      "[epoch:6, iter:450] Loss: 0.371 | Acc: 86.417% \n",
      "Testing...\n",
      "Test Accuracy: 83.890%\n",
      "\n",
      "Epoch: 7\n",
      "[epoch:7, iter:50] Loss: 0.375 | Acc: 86.406% \n",
      "[epoch:7, iter:100] Loss: 0.358 | Acc: 86.773% \n",
      "[epoch:7, iter:150] Loss: 0.352 | Acc: 87.083% \n",
      "[epoch:7, iter:200] Loss: 0.356 | Acc: 87.094% \n",
      "[epoch:7, iter:250] Loss: 0.355 | Acc: 87.181% \n",
      "[epoch:7, iter:300] Loss: 0.357 | Acc: 87.128% \n",
      "[epoch:7, iter:350] Loss: 0.356 | Acc: 87.196% \n",
      "[epoch:7, iter:400] Loss: 0.356 | Acc: 87.164% \n",
      "[epoch:7, iter:450] Loss: 0.353 | Acc: 87.266% \n",
      "Testing...\n",
      "Test Accuracy: 86.010%\n",
      "\n",
      "Epoch: 8\n",
      "[epoch:8, iter:50] Loss: 0.331 | Acc: 87.781% \n",
      "[epoch:8, iter:100] Loss: 0.341 | Acc: 87.422% \n",
      "[epoch:8, iter:150] Loss: 0.344 | Acc: 87.349% \n",
      "[epoch:8, iter:200] Loss: 0.354 | Acc: 87.055% \n",
      "[epoch:8, iter:250] Loss: 0.356 | Acc: 87.006% \n",
      "[epoch:8, iter:300] Loss: 0.356 | Acc: 87.021% \n",
      "[epoch:8, iter:350] Loss: 0.358 | Acc: 87.002% \n",
      "[epoch:8, iter:400] Loss: 0.354 | Acc: 87.197% \n",
      "[epoch:8, iter:450] Loss: 0.352 | Acc: 87.267% \n",
      "Testing...\n",
      "Test Accuracy: 86.220%\n",
      "\n",
      "Epoch: 9\n",
      "[epoch:9, iter:50] Loss: 0.327 | Acc: 88.266% \n",
      "[epoch:9, iter:100] Loss: 0.327 | Acc: 88.055% \n",
      "[epoch:9, iter:150] Loss: 0.333 | Acc: 87.786% \n",
      "[epoch:9, iter:200] Loss: 0.332 | Acc: 87.855% \n",
      "[epoch:9, iter:250] Loss: 0.327 | Acc: 88.013% \n",
      "[epoch:9, iter:300] Loss: 0.326 | Acc: 88.112% \n",
      "[epoch:9, iter:350] Loss: 0.326 | Acc: 88.141% \n",
      "[epoch:9, iter:400] Loss: 0.323 | Acc: 88.234% \n",
      "[epoch:9, iter:450] Loss: 0.323 | Acc: 88.259% \n",
      "Testing...\n",
      "Test Accuracy: 85.090%\n",
      "\n",
      "Epoch: 10\n",
      "[epoch:10, iter:50] Loss: 0.304 | Acc: 88.734% \n",
      "[epoch:10, iter:100] Loss: 0.312 | Acc: 88.430% \n",
      "[epoch:10, iter:150] Loss: 0.317 | Acc: 88.443% \n",
      "[epoch:10, iter:200] Loss: 0.310 | Acc: 88.664% \n",
      "[epoch:10, iter:250] Loss: 0.312 | Acc: 88.656% \n",
      "[epoch:10, iter:300] Loss: 0.307 | Acc: 88.862% \n",
      "[epoch:10, iter:350] Loss: 0.309 | Acc: 88.875% \n",
      "[epoch:10, iter:400] Loss: 0.311 | Acc: 88.748% \n",
      "[epoch:10, iter:450] Loss: 0.313 | Acc: 88.681% \n",
      "Testing...\n",
      "Test Accuracy: 85.460%\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCH = 10\n",
    "for epoch in range(EPOCH):\n",
    "    print('\\nEpoch: %d' % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward & backward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy and loss\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Print every 500th iteration\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% ' %\n",
    "                  (epoch + 1, i + 1, sum_loss / (i + 1), 100. * correct / total))\n",
    "\n",
    "    # Test after each epoch\n",
    "    print('Testing...')\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy: %.3f%%' % (100 * correct / total))\n",
    "\n",
    "print('Training complete.')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
